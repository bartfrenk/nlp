{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB movie rating classifier using BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is mostly a copy of https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime\n",
    "import bert\n",
    "import os\n",
    "import re\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = 'output'\n",
    "DATA_COLUMN = 'sentence'\n",
    "LABEL_COLUMN = 'polarity'\n",
    "LABEL_LIST = [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_folder(folder):\n",
    "    data = {}\n",
    "    data['sentence'] = []\n",
    "    data['sentiment'] = []\n",
    "    for path in os.listdir(folder):\n",
    "        with tf.gfile.GFile(os.path.join(folder, path), 'r') as handle:\n",
    "            data['sentence'].append(handle.read())\n",
    "            data['sentiment'].append(re.match('\\d+_(\\d+)\\.txt', path).group(1))\n",
    "    return pd.DataFrame.from_dict(data)\n",
    "\n",
    "def read_data(folder):\n",
    "    pos_df = read_from_folder(os.path.join(folder, 'pos'))\n",
    "    neg_df = read_from_folder(os.path.join(folder, 'neg'))\n",
    "    pos_df['polarity'] = 1\n",
    "    neg_df['polarity'] = 0\n",
    "    return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "def fetch_data():\n",
    "    path = tf.keras.utils.get_file(\n",
    "        fname='aclImdb.tar.gz',\n",
    "        origin='http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz',\n",
    "        extract=True)\n",
    "    train_df = read_data(os.path.join(os.path.dirname(path), 'aclImdb', 'train'))\n",
    "    test_df = read_data(os.path.join(os.path.dirname(path), 'aclImdb', 'test'))\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average review length train set: 1325.06964\n",
      "Average review length test set: 1293.7924\n",
      "Average numer of words train set: 233.7872\n",
      "Average numer of words test set: 228.52668\n"
     ]
    }
   ],
   "source": [
    "print(\"Average review length train set:\", train.sentence.apply(len).mean())\n",
    "print(\"Average review length test set:\", test.sentence.apply(len).mean())\n",
    "print(\"Average numer of words train set:\", train.sentence.apply(lambda s: len(s.split())).mean())\n",
    "print(\"Average numer of words test set:\", test.sentence.apply(lambda s: len(s.split())).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train: 5000\n",
      "#test: 5000\n"
     ]
    }
   ],
   "source": [
    "train = train.sample(5000)\n",
    "test = test.sample(5000)\n",
    "print(\"#train:\", len(train))\n",
    "print(\"#test:\", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_example(row):\n",
    "    return bert.run_classifier.InputExample(\n",
    "        guid=None,\n",
    "        text_a = row[DATA_COLUMN],\n",
    "        text_b = None,\n",
    "        label = row[LABEL_COLUMN])\n",
    "\n",
    "train_exs = train.apply(create_input_example, axis=1)\n",
    "test_exs = test.apply(create_input_example, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n",
      "INFO:tensorflow:Downloading TF-Hub Module 'https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1'.\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1: 41.62MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1: 51.62MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1: 61.62MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1: 81.62MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1: 91.62MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1: 101.62MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1: 121.62MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1: 141.62MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1: 151.62MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1: 161.62MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1: 181.62MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1: 201.62MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1: 241.62MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1: 261.62MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1: 281.62MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1: 291.62MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1: 311.62MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1: 321.62MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1: 331.62MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1: 341.62MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1: 361.62MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1: 391.62MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1: 411.62MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1: 421.62MB\n",
      "INFO:tensorflow:Downloaded https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1, Total size: 421.86MB\n",
      "INFO:tensorflow:Downloaded TF-Hub Module 'https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1'.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "BERT_MODEL_HUB = 'https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1'\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "        tokenization_info = bert_module(signature='tokenization_info', as_dict=True)\n",
    "        with tf.Session() as sess:\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info['vocab_file'],\n",
    "                                                  tokenization_info['do_lower_case']])\n",
    "    return bert.tokenization.FullTokenizer(\n",
    "        vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'here', \"'\", 's', 'an', 'example', 'of', 'using', 'the', 'bert', 'token', '##izer']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 5000\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] one of the other comment ##ers mentioned that they almost walked out . if i hadn ' t been with my wife , who wanted to stay , i would have left . it ' s a shame , too , because i think it could have been a good movie . but this is easily one of the worst adapted screenplay ##s i ' ve ever seen . it starts out nowhere and it goes nowhere ( i would say it goes nowhere fast , but it really goes nowhere slow . . . painfully slow ) . from time to time there are hints that something interesting might happen , or that there is potentially some depth underneath one of the characters , but [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2028 1997 1996 2060 7615 2545 3855 2008 2027 2471 2939 2041 1012 2065 1045 2910 1005 1056 2042 2007 2026 2564 1010 2040 2359 2000 2994 1010 1045 2052 2031 2187 1012 2009 1005 1055 1037 9467 1010 2205 1010 2138 1045 2228 2009 2071 2031 2042 1037 2204 3185 1012 2021 2023 2003 4089 2028 1997 1996 5409 5967 9000 2015 1045 1005 2310 2412 2464 1012 2009 4627 2041 7880 1998 2009 3632 7880 1006 1045 2052 2360 2009 3632 7880 3435 1010 2021 2009 2428 3632 7880 4030 1012 1012 1012 16267 4030 1007 1012 2013 2051 2000 2051 2045 2024 20385 2008 2242 5875 2453 4148 1010 2030 2008 2045 2003 9280 2070 5995 7650 2028 1997 1996 3494 1010 2021 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] i have to say although i des ##pis ##e these kind of shows , shock horror , i ' m a girl , i feel i have to express my opinion . i had seen dirty sanchez before i saw jack ##ass and think it way sur ##pass ##es jack ##ass in terms of programme making . story lines and interviews are inter weave ##d to create a more interesting show . i saw a few minutes of jack ##ass movie the other night and couldn ' t believe how poorly put together it was , everything just put in a line joke after joke with no relation between anything . it must have been the quick ##est easiest show to edit ever , shocking ##ly [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1045 2031 2000 2360 2348 1045 4078 18136 2063 2122 2785 1997 3065 1010 5213 5469 1010 1045 1005 1049 1037 2611 1010 1045 2514 1045 2031 2000 4671 2026 5448 1012 1045 2018 2464 6530 10568 2077 1045 2387 2990 12054 1998 2228 2009 2126 7505 15194 2229 2990 12054 1999 3408 1997 4746 2437 1012 2466 3210 1998 7636 2024 6970 25308 2094 2000 3443 1037 2062 5875 2265 1012 1045 2387 1037 2261 2781 1997 2990 12054 3185 1996 2060 2305 1998 2481 1005 1056 2903 2129 9996 2404 2362 2009 2001 1010 2673 2074 2404 1999 1037 2240 8257 2044 8257 2007 2053 7189 2090 2505 1012 2009 2442 2031 2042 1996 4248 4355 25551 2265 2000 10086 2412 1010 16880 2135 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 1 (id = 1)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] in my opinion , a guy thing is a hilarious , witty , sexy , romantic , and totally beautiful chick flick that guys will also enjoy . i thought that jason lee and julia stil ##es da ##zzled as a bewildered groom - to - be and his soon - to - be sexy cousin - in - law . if you ask me , they lit up the screen like magic . you can also feel their chemistry between them . before i wrap this up , i ' d like to say that the performances were top grade , the direction was flawless , the production design was nice , the casting was perfect , and the costumes were perfectly designed . in [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1999 2026 5448 1010 1037 3124 2518 2003 1037 26316 1010 25591 1010 7916 1010 6298 1010 1998 6135 3376 14556 17312 2008 4364 2097 2036 5959 1012 1045 2245 2008 4463 3389 1998 6423 25931 2229 4830 17269 2004 1037 24683 18087 1011 2000 1011 2022 1998 2010 2574 1011 2000 1011 2022 7916 5542 1011 1999 1011 2375 1012 2065 2017 3198 2033 1010 2027 5507 2039 1996 3898 2066 3894 1012 2017 2064 2036 2514 2037 6370 2090 2068 1012 2077 1045 10236 2023 2039 1010 1045 1005 1040 2066 2000 2360 2008 1996 4616 2020 2327 3694 1010 1996 3257 2001 27503 1010 1996 2537 2640 2001 3835 1010 1996 9179 2001 3819 1010 1998 1996 12703 2020 6669 2881 1012 1999 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 1 (id = 1)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] what else is left to say ? < br / > < br / > i ' ve read all the reviews here and most are right on . . however , one person even went so far as to call this movie evil and that satan tainted it ( or something along those lines ) . evil ? ! wow , what a shock ##er . . i mean , tb ##n basically made this film . open your eyes please . < br / > < br / > an ##way , this was the very lowest grade of prop ##og ##hand ##a nonsense that has come along in years . < br / > < br / > the most terrifying thing about [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2054 2842 2003 2187 2000 2360 1029 1026 7987 1013 1028 1026 7987 1013 1028 1045 1005 2310 3191 2035 1996 4391 2182 1998 2087 2024 2157 2006 1012 1012 2174 1010 2028 2711 2130 2253 2061 2521 2004 2000 2655 2023 3185 4763 1998 2008 16795 26392 2009 1006 2030 2242 2247 2216 3210 1007 1012 4763 1029 999 10166 1010 2054 1037 5213 2121 1012 1012 1045 2812 1010 26419 2078 10468 2081 2023 2143 1012 2330 2115 2159 3531 1012 1026 7987 1013 1028 1026 7987 1013 1028 2019 4576 1010 2023 2001 1996 2200 7290 3694 1997 17678 8649 11774 2050 14652 2008 2038 2272 2247 1999 2086 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 2087 17082 2518 2055 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] this romantic comedy isn ' t too bad . there are some funny things happening here and there , and there are some rather memorable characters in it . < br / > < br / > the acting , however , is amateur ##ish ( with the exception of the banker ) . while some scenes are great fun , others are simply embarrassing . in particular , i found the \" romantic \" part of the story poor . < br / > < br / > all in all , i guess it ' s worth seeing if you like football and romantic comedies . it ' s not really a bad movie , and the ending did feel quite good . just [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2023 6298 4038 3475 1005 1056 2205 2919 1012 2045 2024 2070 6057 2477 6230 2182 1998 2045 1010 1998 2045 2024 2070 2738 13432 3494 1999 2009 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 3772 1010 2174 1010 2003 5515 4509 1006 2007 1996 6453 1997 1996 13448 1007 1012 2096 2070 5019 2024 2307 4569 1010 2500 2024 3432 16436 1012 1999 3327 1010 1045 2179 1996 1000 6298 1000 2112 1997 1996 2466 3532 1012 1026 7987 1013 1028 1026 7987 1013 1028 2035 1999 2035 1010 1045 3984 2009 1005 1055 4276 3773 2065 2017 2066 2374 1998 6298 22092 1012 2009 1005 1055 2025 2428 1037 2919 3185 1010 1998 1996 4566 2106 2514 3243 2204 1012 2074 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "INFO:tensorflow:Writing example 0 of 5000\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] ok i am a huge tr ##ac ##i fan so her just being in the movie automatically makes it rank at the 8 . 5 + rating . but even besides her being in it i thought it was a good movie especially for it being an hbo movie . but i am afraid if you take tr ##ac ##i out of the movie it would just be ok . but a person can ' t do that she is in it and she is a wonderful actress . she just keeps getting better and better . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 7929 1045 2572 1037 4121 19817 6305 2072 5470 2061 2014 2074 2108 1999 1996 3185 8073 3084 2009 4635 2012 1996 1022 1012 1019 1009 5790 1012 2021 2130 4661 2014 2108 1999 2009 1045 2245 2009 2001 1037 2204 3185 2926 2005 2009 2108 2019 14633 3185 1012 2021 1045 2572 4452 2065 2017 2202 19817 6305 2072 2041 1997 1996 3185 2009 2052 2074 2022 7929 1012 2021 1037 2711 2064 1005 1056 2079 2008 2016 2003 1999 2009 1998 2016 2003 1037 6919 3883 1012 2016 2074 7906 2893 2488 1998 2488 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 1 (id = 1)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] this production was quite a surprise for me . i absolutely love obscure early 30 ##s movies , but i wasn ' t prepared for the last 25 minutes of this story . if , by any chance , you ' re not convinced in the first half , hang in there for the finale . of course , you must look at the b ##lat ##ant racism as being purely topical . a fascinating viewing experience , but i think the cat ' s paw is not available on video / dvd yet . watch your pbs listings ! [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2023 2537 2001 3243 1037 4474 2005 2033 1012 1045 7078 2293 14485 2220 2382 2015 5691 1010 2021 1045 2347 1005 1056 4810 2005 1996 2197 2423 2781 1997 2023 2466 1012 2065 1010 2011 2151 3382 1010 2017 1005 2128 2025 6427 1999 1996 2034 2431 1010 6865 1999 2045 2005 1996 9599 1012 1997 2607 1010 2017 2442 2298 2012 1996 1038 20051 4630 14398 2004 2108 11850 25665 1012 1037 17160 10523 3325 1010 2021 1045 2228 1996 4937 1005 1055 22195 2003 2025 2800 2006 2678 1013 4966 2664 1012 3422 2115 13683 26213 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 1 (id = 1)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] continuing with the exclusive film programme about complicated relationships in some european courts , last night in the sc ##hl ##oss theatre was shown \" anna bo ##ley ##n \" , a film directed by the great te ##uto ##nic film director herr ernst lu ##bit ##sch . the film depicts the terrible story of the queen consort of the british king henry viii . she was executed by her husband ( well , not exactly , the king ordered the execution ##ers to do his dirty work ) not to mention that this marriage caused an important political and religious historical event , the english reformation . < br / > < br / > the film stars dame hen ##ny porte ##n , germany [SEP]\n",
      "INFO:tensorflow:input_ids: 101 5719 2007 1996 7262 2143 4746 2055 8552 6550 1999 2070 2647 5434 1010 2197 2305 1999 1996 8040 7317 15094 3004 2001 3491 1000 4698 8945 3051 2078 1000 1010 1037 2143 2856 2011 1996 2307 8915 16161 8713 2143 2472 23506 10728 11320 16313 11624 1012 1996 2143 11230 1996 6659 2466 1997 1996 3035 13440 1997 1996 2329 2332 2888 9937 1012 2016 2001 6472 2011 2014 3129 1006 2092 1010 2025 3599 1010 1996 2332 3641 1996 7781 2545 2000 2079 2010 6530 2147 1007 2025 2000 5254 2008 2023 3510 3303 2019 2590 2576 1998 3412 3439 2724 1010 1996 2394 13708 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 2143 3340 8214 21863 4890 25927 2078 1010 2762 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 1 (id = 1)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] wayne wang ' s direction may be the ingredient which made this film much more impressive to me than \" sl ##ums of beverly hills \" , which covers remarkably similar ground . the inter ##play between susan sara ##ndon and natalie port ##man is ri ##vet ##ing . real chemistry there . this film succeeded in bringing me inside the dysfunction ##al life of these two women without dragging me down into depressed frustration . susan sara ##ndon ' s character hammer ##s at all the nerves which a na ##rc ##iss ##istic parent is capable of touching in an ins ##ecure adolescent . she amazingly manages to do this without coming across as fl ##ori ##dly insane or intentionally sad ##istic . and , [SEP]\n",
      "INFO:tensorflow:input_ids: 101 6159 7418 1005 1055 3257 2089 2022 1996 21774 2029 2081 2023 2143 2172 2062 8052 2000 2033 2084 1000 22889 18163 1997 12218 4564 1000 1010 2029 4472 17431 2714 2598 1012 1996 6970 13068 2090 6294 7354 19333 1998 10829 3417 2386 2003 15544 19510 2075 1012 2613 6370 2045 1012 2023 2143 4594 1999 5026 2033 2503 1996 28466 2389 2166 1997 2122 2048 2308 2302 11920 2033 2091 2046 14777 9135 1012 6294 7354 19333 1005 1055 2839 8691 2015 2012 2035 1996 10627 2029 1037 6583 11890 14643 6553 6687 2003 5214 1997 7244 1999 2019 16021 29150 20274 1012 2016 29350 9020 2000 2079 2023 2302 2746 2408 2004 13109 10050 18718 9577 2030 15734 6517 6553 1012 1998 1010 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 1 (id = 1)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] i remember watching this for the first time in the 80 ' s as a teen . man , i ' ve read the reviews on this trash and i find myself astonished by the voting . this movie does not deserve four stars ! ! ! this movie is not better than top ##gun . top ##gun has its own problems ; don ' t get me wrong . this movie should be banned just for its own stupidity . so many stereotypes , so many loop holes , so much poor dial ##og . i cannot think of one red ##eem ##ing quality of this vomit . this is not action / adventure . this is a bad joke on film . kinda like [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1045 3342 3666 2023 2005 1996 2034 2051 1999 1996 3770 1005 1055 2004 1037 9458 1012 2158 1010 1045 1005 2310 3191 1996 4391 2006 2023 11669 1998 1045 2424 2870 22741 2011 1996 6830 1012 2023 3185 2515 2025 10107 2176 3340 999 999 999 2023 3185 2003 2025 2488 2084 2327 12734 1012 2327 12734 2038 2049 2219 3471 1025 2123 1005 1056 2131 2033 3308 1012 2023 3185 2323 2022 7917 2074 2005 2049 2219 28072 1012 2061 2116 22807 1010 2061 2116 7077 8198 1010 2061 2172 3532 13764 8649 1012 1045 3685 2228 1997 2028 2417 21564 2075 3737 1997 2023 23251 1012 2023 2003 2025 2895 1013 6172 1012 2023 2003 1037 2919 8257 2006 2143 1012 17704 2066 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LENGTH = 128\n",
    "train_features = bert.run_classifier.convert_examples_to_features(train_exs, LABEL_LIST, MAX_SEQ_LENGTH, tokenizer)\n",
    "test_features = bert.run_classifier.convert_examples_to_features(test_exs, LABEL_LIST, MAX_SEQ_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels, num_labels):\n",
    "    \"\"\"Create the model by stacking a fully connected layer on top of a pretrained\n",
    "    BERT model from Tensorflow hub. The input of the BERT model is sequence of\n",
    "    tokens, the output is a sequence representation (due to 'pooled_output').\n",
    "    \n",
    "    Adding an extra layer is just using standard Tensorflow functionality. \n",
    "    \"\"\"\n",
    "    \n",
    "    bert_module = hub.Module(\n",
    "        BERT_MODEL_HUB,\n",
    "        trainable=True)\n",
    "    bert_inputs = dict(\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids)\n",
    "    bert_outputs = bert_module(\n",
    "        inputs=bert_inputs,\n",
    "        signature='tokens',\n",
    "        as_dict=True)\n",
    "    \n",
    "    output_layer = bert_outputs['pooled_output']\n",
    "    \n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "    \n",
    "    output_weights = tf.get_variable(\n",
    "        'output_weights', [num_labels, hidden_size],\n",
    "        initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    \n",
    "    output_bias = tf.get_variable(\n",
    "        'output_bias', [num_labels], initializer=tf.zeros_initializer())\n",
    "    \n",
    "    with tf.variable_scope('loss'):\n",
    "        output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "        \n",
    "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "        \n",
    "        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "        \n",
    "        predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "        if is_predicting:\n",
    "            return (predicted_labels, log_probs)\n",
    "        \n",
    "        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "        return (loss, predicted_labels, log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn_builder(num_labels, learning_rate, num_train_steps, num_warmup_steps):\n",
    "    \n",
    "    def model_fn(features, labels, mode, params):\n",
    "        input_ids = features['input_ids']\n",
    "        input_mask = features['input_mask']\n",
    "        segment_ids = features['segment_ids']\n",
    "        label_ids = features['label_ids']\n",
    "        \n",
    "        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "        \n",
    "        if not is_predicting:\n",
    "            (loss, predicted_labels, log_probs) = create_model(\n",
    "                is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "            \n",
    "            train_op = bert.optimization.create_optimizer(\n",
    "                loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
    "            \n",
    "            def metric_fn(label_ids, predicted_labels):\n",
    "                accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "                f1_score = tf.contrib.metrics.f1_score(label_ids, predicted_labels)\n",
    "                auc = tf.metrics.auc(label_ids, predicted_labels)\n",
    "                recall = tf.metrics.recall(label_ids, predicted_labels)\n",
    "                precision = tf.metrics.precision(label_ids, predicted_labels)\n",
    "                true_pos = tf.metrics.true_positives(label_ids, predicted_labels)\n",
    "                true_neg = tf.metrics.true_negatives(label_ids, predicted_labels)\n",
    "                false_pos = tf.metrics.false_positives(label_ids, predicted_labels)\n",
    "                false_neg = tf.metrics.false_negatives(label_ids, predicted_labels)\n",
    "                return {'eval_accuracy': accuracy,\n",
    "                        'f1_score': f1_score,\n",
    "                        'auc': auc,\n",
    "                        'precision': precision,\n",
    "                        'recall': recall,\n",
    "                        'true_positives': true_pos,\n",
    "                        'true_negatives': true_neg,\n",
    "                        'false_positives': false_pos,\n",
    "                        'false_negatives': false_neg}\n",
    "    \n",
    "            eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "            \n",
    "            if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "                return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "            else:\n",
    "                return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metrics)\n",
    "        else:\n",
    "            (predicted_labels, log_probs) = create_model(\n",
    "                is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "            predictions = {\n",
    "                'probabilities': log_probs,\n",
    "                'labels': predicted_labels}\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "        \n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 3.0\n",
    "WARMUP_PROPORTION = 0.1\n",
    "SAVE_CHECKPOINTS_STEPS = 500\n",
    "SAVE_SUMMARY_STEPS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f387896e358>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "model_fn = model_fn_builder( \n",
    "    num_labels=len(LABEL_LIST),\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_steps=num_train_steps,\n",
    "    num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "    model_fn=model_fn,\n",
    "    config=run_config,\n",
    "    params={'batch_size': BATCH_SIZE}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = bert.run_classifier.input_fn_builder(\n",
    "    features=train_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=True,\n",
    "    drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train...\n",
      "INFO:tensorflow:Skipping training since max_steps has already saved.\n",
      "Training took time  0:00:00.005710\n"
     ]
    }
   ],
   "source": [
    "print('Starting to train...')\n",
    "current_time = datetime.now()\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print('Training took time ', datetime.now() - current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_fn = bert.run_classifier.input_fn_builder(\n",
    "    features=test_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-02-17-21:35:43\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from output/model.ckpt-468\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-02-17-22:21:48\n",
      "INFO:tensorflow:Saving dict for global step 468: auc = 0.86803305, eval_accuracy = 0.868, f1_score = 0.866017, false_negatives = 372.0, false_positives = 288.0, global_step = 468, loss = 0.47416788, precision = 0.8810409, recall = 0.851497, true_negatives = 2207.0, true_positives = 2133.0\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 468: output/model.ckpt-468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'auc': 0.86803305,\n",
       " 'eval_accuracy': 0.868,\n",
       " 'f1_score': 0.866017,\n",
       " 'false_negatives': 372.0,\n",
       " 'false_positives': 288.0,\n",
       " 'loss': 0.47416788,\n",
       " 'precision': 0.8810409,\n",
       " 'recall': 0.851497,\n",
       " 'true_negatives': 2207.0,\n",
       " 'true_positives': 2133.0,\n",
       " 'global_step': 468}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.evaluate(input_fn=test_input_fn, steps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(in_sentences):\n",
    "    labels = ['Negative', 'Positive']\n",
    "    input_examples = [create_input_example({DATA_COLUMN: x, LABEL_COLUMN: 0})\n",
    "                                           for x in in_sentences]\n",
    "    input_features = bert.run_classifier.convert_examples_to_features(\n",
    "        input_examples, LABEL_LIST, MAX_SEQ_LENGTH, tokenizer)\n",
    "    predict_input_fn = bert.run_classifier.input_fn_builder(\n",
    "        features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
    "    predictions = estimator.predict(predict_input_fn)\n",
    "    return [(sentence, prediction['probabilities'], labels[prediction['labels']])\n",
    "            for sentence, prediction in zip(in_sentences, predictions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'That movie was absolutely awful',\n",
    "    'The acting was a bit lacking',\n",
    "    'The film was creative and surprising',\n",
    "    'Absolutely fantastic'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 4\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] that movie was absolutely awful [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2008 3185 2001 7078 9643 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] the acting was a bit lacking [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1996 3772 2001 1037 2978 11158 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] the film was creative and surprising [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1996 2143 2001 5541 1998 11341 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: None\n",
      "INFO:tensorflow:tokens: [CLS] absolutely fantastic [SEP]\n",
      "INFO:tensorflow:input_ids: 101 7078 10392 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from output/model.ckpt-468\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "predictions = get_predictions(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('That movie was absolutely awful',\n",
       "  array([-1.9294472e-03, -6.2514567e+00], dtype=float32),\n",
       "  'Negative'),\n",
       " ('The acting was a bit lacking',\n",
       "  array([-3.1934001e-03, -5.7482734e+00], dtype=float32),\n",
       "  'Negative'),\n",
       " ('The film was creative and surprising',\n",
       "  array([-5.2045507, -0.0055067], dtype=float32),\n",
       "  'Positive'),\n",
       " ('Absolutely fantastic',\n",
       "  array([-4.8700013, -0.007703 ], dtype=float32),\n",
       "  'Positive')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv (bert)",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
